{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.8/792.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 10662\n",
      "Categories: ['neg', 'pos']\n",
      "First four sentences:\n",
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "\n",
    "# Download the necessary NLTK data if you haven't already\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the movie review sentences from the NLTK corpus\n",
    "sentences = sentence_polarity.sents()\n",
    "documents = [(sent, category) for category in sentence_polarity.categories() \n",
    "             for sent in sentence_polarity.sents(categories=category)]\n",
    "random.shuffle(documents)  # Shuffle the documents\n",
    "\n",
    "# Check the number of sentences and categories\n",
    "print(f\"Total number of sentences: {len(sentences)}\")\n",
    "print(f\"Categories: {sentence_polarity.categories()}\")\n",
    "print(\"First four sentences:\")\n",
    "for sent in sentences[:4]:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with BOW features: 75.80%\n",
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     19.8 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     16.9 : 1.0\n",
      "               V_generic = True              neg : pos    =     16.2 : 1.0\n",
      "                  V_flat = True              neg : pos    =     14.9 : 1.0\n",
      "               V_routine = True              neg : pos    =     14.9 : 1.0\n",
      "             V_inventive = True              pos : neg    =     14.4 : 1.0\n",
      "                V_boring = True              neg : pos    =     12.9 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     12.7 : 1.0\n",
      "                  V_dull = True              neg : pos    =     12.5 : 1.0\n",
      "             V_affecting = True              pos : neg    =     12.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Extract all words and get the 2000 most common words for BOW features\n",
    "all_words_list = [word.lower() for (sent, _) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_features = [word for (word, _) in all_words.most_common(2000)]\n",
    "\n",
    "# Define a function for BOW features\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {f'V_{word}': (word in document_words) for word in word_features}\n",
    "    return features\n",
    "\n",
    "# Create feature sets using BOW\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]\n",
    "\n",
    "# Split into training and testing sets (90/10 split)\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "\n",
    "# Train a Naive Bayes classifier and evaluate it\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f'Accuracy with BOW features: {accuracy * 100:.2f}%')\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6885 words from the subjectivity lexicon.\n",
      "Accuracy with Subjectivity Lexicon features: 75.80%\n"
     ]
    }
   ],
   "source": [
    "# Import the readSubjectivity function from Subjectivity.py (ensure the file is in the same directory)\n",
    "from subjectivity import readSubjectivity  # Adjust the path if necessary\n",
    "\n",
    "# Load the Subjectivity Lexicon\n",
    "SLpath = \"subjclueslen1-HLTEMNLP05.tff\"  # Path to the subjectivity lexicon file\n",
    "SL = readSubjectivity(SLpath)\n",
    "print(f\"Loaded {len(SL)} words from the subjectivity lexicon.\")\n",
    "\n",
    "# Define features using the Subjectivity Lexicon\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {f'V_{word}': (word in document_words) for word in word_features}\n",
    "    weakPos, strongPos, weakNeg, strongNeg = 0, 0, 0, 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, _, _, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            elif strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            elif strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            elif strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "    features['positivecount'] = weakPos + (2 * strongPos)\n",
    "    features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "    return features\n",
    "\n",
    "# Create feature sets using the Subjectivity Lexicon features\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in documents]\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "\n",
    "# Train and evaluate the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f'Accuracy with Subjectivity Lexicon features: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'rather', 'hardly', 'scarcely', \n",
    "                 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "# Define features with negation handling\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {f'V_{word}': False for word in word_features}\n",
    "    features.update({f'V_NOT{word}': False for word in word_features})\n",
    "    for i in range(len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            if document[i] in word_features:\n",
    "                features[f'V_NOT{document[i]}'] = True\n",
    "        elif word in word_features:\n",
    "            features[f'V_{word}'] = True\n",
    "    return features\n",
    "\n",
    "# Create feature sets using negation handling\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "\n",
    "# Train and evaluate the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f'Accuracy with Negation features: {accuracy * 100:.2f}%')\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
