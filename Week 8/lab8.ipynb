{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 10662\n",
      "Categories: ['neg', 'pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the movie review sentences from the NLTK corpus\n",
    "documents = [(sent, category) for category in sentence_polarity.categories() \n",
    "             for sent in sentence_polarity.sents(categories=category)]\n",
    "random.shuffle(documents)  # Shuffle the documents\n",
    "\n",
    "# Check the total number of sentences and categories\n",
    "print(f\"Total number of sentences: {len(documents)}\")\n",
    "print(f\"Categories: {sentence_polarity.categories()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy with BOW features: 76.20%\n",
      "Most Informative Features\n",
      "            V_engrossing = True              pos : neg    =     19.7 : 1.0\n",
      "                 V_flaws = True              pos : neg    =     15.7 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.6 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     15.6 : 1.0\n",
      "               V_routine = True              neg : pos    =     14.3 : 1.0\n",
      "                V_flawed = True              pos : neg    =     13.7 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     13.7 : 1.0\n",
      "                  V_flat = True              neg : pos    =     13.4 : 1.0\n",
      "                V_boring = True              neg : pos    =     13.3 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     12.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Extract all words and get the 2000 most common words for BOW features\n",
    "all_words_list = [word.lower() for (sent, _) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_features = [word for (word, _) in all_words.most_common(2000)]\n",
    "\n",
    "# Define a function for BOW features\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {f'V_{word}': (word in document_words) for word in word_features}\n",
    "    return features\n",
    "\n",
    "# Create feature sets using BOW\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]\n",
    "\n",
    "# Split into training and testing sets (90/10 split)\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "\n",
    "# Train a Naive Bayes classifier and evaluate it\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "baseline_accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f'Baseline Accuracy with BOW features: {baseline_accuracy * 100:.2f}%')\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6885 words from the subjectivity lexicon.\n",
      "Improved Accuracy with Subjectivity Lexicon features: 77.40%\n"
     ]
    }
   ],
   "source": [
    "# Import the readSubjectivity function from Subjectivity.py (ensure the file is in the same directory)\n",
    "from subjectivity import readSubjectivity  # Adjust the path if necessary\n",
    "\n",
    "# Load the Subjectivity Lexicon\n",
    "SLpath = \"subjclueslen1-HLTEMNLP05.tff\"  # Path to the subjectivity lexicon file\n",
    "SL = readSubjectivity(SLpath)\n",
    "print(f\"Loaded {len(SL)} words from the subjectivity lexicon.\")\n",
    "\n",
    "# Define features using the Subjectivity Lexicon\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {f'V_{word}': (word in document_words) for word in word_features}\n",
    "    weakPos, strongPos, weakNeg, strongNeg = 0, 0, 0, 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, _, _, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            elif strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            elif strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            elif strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "    features['positivecount'] = weakPos + (2 * strongPos)\n",
    "    features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "    return features\n",
    "\n",
    "# Create feature sets using the Subjectivity Lexicon features\n",
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in documents]\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "\n",
    "# Train and evaluate the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "improved_accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f'Improved Accuracy with Subjectivity Lexicon features: {improved_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'rather', 'hardly', 'scarcely', \n",
    "                 'rarely', 'seldom', 'neither', 'nor', \"n't\"]\n",
    "# Define the features with negation handling\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {f'V_{word}': False for word in word_features}\n",
    "    features.update({f'V_NOT{word}': False for word in word_features})\n",
    "    \n",
    "    negated = False\n",
    "    for word in document:\n",
    "        if word in negationwords or word.endswith(\"n't\"):\n",
    "            negated = True\n",
    "            continue\n",
    "        if word in word_features:\n",
    "            if negated:\n",
    "                features[f'V_NOT{word}'] = True\n",
    "                negated = False\n",
    "            else:\n",
    "                features[f'V_{word}'] = True\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate features in smaller batches\n",
    "def generate_featuresets_in_batches(documents, word_features, negationwords, batch_size=100):\n",
    "    for start in range(0, len(documents), batch_size):\n",
    "        batch = documents[start:start + batch_size]\n",
    "        yield [(NOT_features(d, word_features, negationwords), c) for (d, c) in batch]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process the data in smaller batches to create the training and testing sets\n",
    "NOT_featuresets = []\n",
    "for batch in generate_featuresets_in_batches(documents, word_features, negationwords):\n",
    "    NOT_featuresets.extend(batch)\n",
    "\n",
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Negation features: 75.50%\n",
      "Most Informative Features\n",
      "                V_boring = True              neg : pos    =     30.3 : 1.0\n",
      "            V_engrossing = True              pos : neg    =     19.7 : 1.0\n",
      "                  V_warm = True              pos : neg    =     19.0 : 1.0\n",
      "                  V_dull = True              neg : pos    =     17.8 : 1.0\n",
      "             V_NOTenough = True              neg : pos    =     16.3 : 1.0\n",
      "                 V_flaws = True              pos : neg    =     15.7 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.6 : 1.0\n",
      "              V_mediocre = True              neg : pos    =     15.6 : 1.0\n",
      "            V_unexpected = True              pos : neg    =     15.0 : 1.0\n",
      "                 V_fails = True              neg : pos    =     15.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f'Accuracy with Negation features: {accuracy * 100:.2f}%')\n",
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
