{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentence_polarity\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('sentence_polarity')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load and shuffle the data\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories()\n",
    "             for sent in sentence_polarity.sents(categories=cat)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Extract the top 1500 words as unigram features\n",
    "all_words_list = [word for (sent, cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(1500)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "\n",
    "# Define a unigram feature extraction function\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Create unigram feature sets\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Unigram Accuracy: 0.716\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "\n",
    "# Train the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"Baseline Unigram Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Bigram Accuracy: 0.716\n"
     ]
    }
   ],
   "source": [
    "# Generate bigram features\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)  # Top 500 bigrams\n",
    "\n",
    "# Define a bigram feature extraction function\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)\n",
    "    return features\n",
    "\n",
    "# Create bigram feature sets\n",
    "bigram_featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in documents]\n",
    "\n",
    "# Train and evaluate with bigram features\n",
    "train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(\"Baseline Bigram Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_features(document, word_features):\n",
    "    # Ensure the document is tokenized correctly\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document, tagset='universal')  # Use Universal Tagset\n",
    "    \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    \n",
    "    # Initialize counts for POS tags\n",
    "    numNoun = numVerb = numAdj = numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag == 'NOUN': numNoun += 1\n",
    "        if tag == 'VERB': numVerb += 1\n",
    "        if tag == 'ADJ': numAdj += 1\n",
    "        if tag == 'ADV': numAdverb += 1\n",
    "    \n",
    "    # Add counts to features\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unigram_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Generate feature sets\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m unigram_featuresets \u001b[38;5;241m=\u001b[39m [(\u001b[43munigram_features\u001b[49m(d, word_features), c) \u001b[38;5;28;01mfor\u001b[39;00m (d, c) \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m     29\u001b[0m bigram_featuresets \u001b[38;5;241m=\u001b[39m [(bigram_features_func(d, word_features, bigram_features), c) \u001b[38;5;28;01mfor\u001b[39;00m (d, c) \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m     30\u001b[0m POS_featuresets \u001b[38;5;241m=\u001b[39m [(custom_POS_features(d, word_features), c) \u001b[38;5;28;01mfor\u001b[39;00m (d, c) \u001b[38;5;129;01min\u001b[39;00m documents]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unigram_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Define custom POS feature extraction\n",
    "def custom_POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {'contains({})'.format(word): (word in document_words) for word in word_features}\n",
    "\n",
    "    # Custom suffix-based POS tagging\n",
    "    suffix_tags = Counter()\n",
    "    for word in document:\n",
    "        if word.endswith('ing'):  # Assume gerund/verb\n",
    "            suffix_tags['verbs'] += 1\n",
    "        elif word.endswith('ly'):  # Assume adverb\n",
    "            suffix_tags['adverbs'] += 1\n",
    "        elif word.endswith('ed'):  # Assume past-tense verb\n",
    "            suffix_tags['verbs'] += 1\n",
    "        elif word.endswith('ous') or word.endswith('able') or word.endswith('ive'):  # Assume adjective\n",
    "            suffix_tags['adjectives'] += 1\n",
    "        elif word.isalpha() and len(word) > 5:  # Assume longer words as nouns\n",
    "            suffix_tags['nouns'] += 1\n",
    "\n",
    "    # Add POS counts to features\n",
    "    features['nouns'] = suffix_tags['nouns']\n",
    "    features['verbs'] = suffix_tags['verbs']\n",
    "    features['adjectives'] = suffix_tags['adjectives']\n",
    "    features['adverbs'] = suffix_tags['adverbs']\n",
    "    return features\n",
    "\n",
    "# Generate feature sets\n",
    "unigram_featuresets = [(unigram_features(d, word_features), c) for (d, c) in documents]\n",
    "bigram_featuresets = [(bigram_features_func(d, word_features, bigram_features), c) for (d, c) in documents]\n",
    "POS_featuresets = [(custom_POS_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation function\n",
    "def cross_validation_accuracy(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets) / num_folds)\n",
    "    accuracy_list = []\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[i * subset_size:][:subset_size]\n",
    "        train_this_round = featuresets[:i * subset_size] + featuresets[(i + 1) * subset_size:]\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print(f\"Fold {i + 1} Accuracy:\", accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    print(\"Mean Accuracy:\", sum(accuracy_list) / num_folds)\n",
    "\n",
    "# Cross-validation for unigrams\n",
    "print(\"\\nUnigram Cross-Validation (5-fold):\")\n",
    "cross_validation_accuracy(5, featuresets)\n",
    "\n",
    "# Cross-validation for bigrams\n",
    "print(\"\\nBigram Cross-Validation (5-fold):\")\n",
    "cross_validation_accuracy(5, bigram_featuresets)\n",
    "\n",
    "# Cross-validation for POS features\n",
    "print(\"\\nPOS Cross-Validation (5-fold):\")\n",
    "cross_validation_accuracy(5, POS_featuresets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def eval_measures(gold, predicted):\n",
    "    labels = list(set(gold))\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    for lab in labels:\n",
    "        TP = FP = FN = 0\n",
    "        for i in range(len(gold)):\n",
    "            if gold[i] == lab and predicted[i] == lab: TP += 1\n",
    "            if gold[i] != lab and predicted[i] == lab: FP += 1\n",
    "            if gold[i] == lab and predicted[i] != lab: FN += 1\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        print(f\"{lab}\\t{precision:.3f}\\t\\t{recall:.3f}\\t\\t{f1:.3f}\")\n",
    "\n",
    "# Evaluate using test set\n",
    "goldlist = [label for (_, label) in test_set]\n",
    "predictedlist = [classifier.classify(features) for (features, _) in test_set]\n",
    "eval_measures(goldlist, predictedlist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
