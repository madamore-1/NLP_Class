{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% Import Libraries\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% File paths\n",
    "TRAIN_FILE = './corpus/train.tsv'\n",
    "LIWC_FILE = './SentimentLexicons/liwcdic2007.dic'\n",
    "\n",
    "# Load LIWC Words\n",
    "def read_words(file_path):\n",
    "    poslist, neglist = [], []\n",
    "    with open(file_path, encoding='latin1') as flexicon:\n",
    "        wordlines = [line.strip() for line in flexicon]\n",
    "        for line in wordlines:\n",
    "            if not line == '':\n",
    "                items = line.split()\n",
    "                word, classes = items[0], items[1:]\n",
    "                for c in classes:\n",
    "                    if c == '126':  # Positive emotion\n",
    "                        poslist.append(word)\n",
    "                    elif c == '127':  # Negative emotion\n",
    "                        neglist.append(word)\n",
    "    return poslist, neglist\n",
    "liwc_pos, liwc_neg = read_words(LIWC_FILE)\n",
    "\n",
    "# Load Data\n",
    "def load_data(file_path, limit=None):\n",
    "    phrases, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        next(file)  # Skip header\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 4:\n",
    "                phrases.append(parts[2])  # Phrase text\n",
    "                labels.append(int(parts[3]))  # Sentiment label\n",
    "                if limit and len(phrases) >= limit:\n",
    "                    break\n",
    "    return phrases, labels\n",
    "\n",
    "# Preprocess Text\n",
    "def preprocess_text(text):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "# Bag-of-Words Features\n",
    "def bag_of_words_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    return {'contains({})'.format(word): (word in document_words) for word in word_features}\n",
    "\n",
    "# LIWC Features\n",
    "def liwc_features(document):\n",
    "    pos_count = sum(1 for word in document if word in liwc_pos)\n",
    "    neg_count = sum(1 for word in document if word in liwc_neg)\n",
    "    return {'LIWC_positive': pos_count, 'LIWC_negative': neg_count}\n",
    "\n",
    "# Combined Features\n",
    "def combined_features(document, word_features):\n",
    "    features = bag_of_words_features(document, word_features)\n",
    "    features.update(liwc_features(document))\n",
    "    return features\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load and preprocess data\n",
    "train_phrases, train_labels = load_data(TRAIN_FILE, limit=10000)\n",
    "train_phrases = [preprocess_text(phrase) for phrase in train_phrases]\n",
    "\n",
    "# Extract Bag-of-Words Features\n",
    "all_words = [word for phrase in train_phrases for word in phrase]\n",
    "word_features = list(nltk.FreqDist(all_words))[:1500]\n",
    "\n",
    "# Create Combined Features\n",
    "combined_feature_sets = [\n",
    "    (combined_features(phrase, word_features), label)\n",
    "    for phrase, label in zip(train_phrases, train_labels)\n",
    "]\n",
    "\n",
    "# Convert Features to Scikit-Learn Format\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform([features for features, label in combined_feature_sets])\n",
    "y = [label for _, label in combined_feature_sets]\n",
    "\n",
    "# Handle Class Imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_depth': 30, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, scoring='f1_macro', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Confusion Matrix:\n",
      "[[864  87 224   0   5]\n",
      " [134 591 280  32  18]\n",
      " [ 55 180 676 140  81]\n",
      " [ 27  37 292 648 153]\n",
      " [  5   6 202 103 798]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76      1180\n",
      "           1       0.66      0.56      0.60      1055\n",
      "           2       0.40      0.60      0.48      1132\n",
      "           3       0.70      0.56      0.62      1157\n",
      "           4       0.76      0.72      0.74      1114\n",
      "\n",
      "    accuracy                           0.63      5638\n",
      "   macro avg       0.66      0.63      0.64      5638\n",
      "weighted avg       0.66      0.63      0.64      5638\n",
      "\n",
      "\n",
      "Random Forest Results:\n",
      "Confusion Matrix:\n",
      "[[960  70 142   3   5]\n",
      " [ 76 693 239  33  14]\n",
      " [ 16 116 870 116  14]\n",
      " [  6  33 259 754 105]\n",
      " [  9   7  99  75 924]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85      1180\n",
      "           1       0.75      0.66      0.70      1055\n",
      "           2       0.54      0.77      0.63      1132\n",
      "           3       0.77      0.65      0.71      1157\n",
      "           4       0.87      0.83      0.85      1114\n",
      "\n",
      "    accuracy                           0.75      5638\n",
      "   macro avg       0.77      0.74      0.75      5638\n",
      "weighted avg       0.77      0.75      0.75      5638\n",
      "\n",
      "\n",
      "Naive Bayes Accuracy: 0.6245\n"
     ]
    }
   ],
   "source": [
    "# %% Logistic Regression\n",
    "clf_logreg = LogisticRegression(class_weight='balanced', max_iter=500, solver='lbfgs', multi_class='multinomial')\n",
    "clf_logreg.fit(X_train, y_train)\n",
    "predictions_logreg = clf_logreg.predict(X_test)\n",
    "print(\"Logistic Regression Results:\")\n",
    "evaluate_model(y_test, predictions_logreg)\n",
    "\n",
    "# %% Random Forest\n",
    "clf_rf = RandomForestClassifier(class_weight='balanced', n_estimators=100)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "predictions_rf = clf_rf.predict(X_test)\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "evaluate_model(y_test, predictions_rf)\n",
    "\n",
    "# %% Naive Bayes\n",
    "random.shuffle(combined_feature_sets)\n",
    "train_size = int(0.8 * len(combined_feature_sets))\n",
    "train_set, test_set = combined_feature_sets[:train_size], combined_feature_sets[train_size:]\n",
    "classifier_nb = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"\\nNaive Bayes Accuracy:\", nltk.classify.util.accuracy(classifier_nb, test_set))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
